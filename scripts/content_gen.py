#!/usr/bin/env python3
"""
å°çº¢ä¹¦ AI å†…å®¹ç”Ÿæˆå™¨
ä½¿ç”¨ OpenClaw å·²é…ç½®çš„ LLMï¼ˆç™¾ç‚¼ Qwenï¼‰ç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„æ ‡é¢˜ã€æ­£æ–‡å’Œæ ‡ç­¾ã€‚
æ”¯æŒå¤šç§æ–‡æ¡ˆé£æ ¼æ¨¡æ¿ã€‚
"""

import os
import sys
import json
import argparse
import re
from pathlib import Path
from datetime import datetime

# è·¯å¾„å¸¸é‡
SKILL_DIR = Path(__file__).parent.parent
TEMPLATES_DIR = SKILL_DIR / 'templates'
CONTENT_DIR = SKILL_DIR / 'content'
OPENCLAW_CONFIG = Path.home() / '.openclaw' / 'openclaw.json'

CONTENT_DIR.mkdir(exist_ok=True)


def load_config():
    """åŠ è½½ OpenClaw é…ç½®ï¼Œè·å– API ä¿¡æ¯"""
    try:
        with open(OPENCLAW_CONFIG, 'r') as f:
            return json.load(f)
    except Exception:
        return {}


def get_llm_config():
    """è·å– LLM API é…ç½®ï¼ˆä¼˜å…ˆ Gemini å…è´¹ APIï¼Œé™çº§ç™¾ç‚¼ï¼‰"""
    cfg = load_config()
    providers = cfg.get('models', {}).get('providers', {})

    # ä¼˜å…ˆç”¨ Geminiï¼ˆå…è´¹ï¼‰
    gemini_key = None
    try:
        gemini_key = cfg['skills']['entries']['nano-banana-pro']['apiKey']
    except (KeyError, TypeError):
        pass
    if not gemini_key:
        gemini_key = os.environ.get('GEMINI_API_KEY', '')

    if gemini_key:
        return {
            'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai',
            'api_key': gemini_key,
            'model': 'gemini-2.5-flash',
            'api_type': 'openai-completions',
            'proxy': 'http://127.0.0.1:7897',
        }

    # é™çº§ç™¾ç‚¼
    if 'bailian' in providers:
        p = providers['bailian']
        model_id = p.get('models', [{}])[0].get('id', 'qwen-plus') if p.get('models') else 'qwen-plus'
        return {
            'base_url': p.get('baseUrl', ''),
            'api_key': p.get('apiKey', ''),
            'model': model_id,
            'api_type': p.get('api', 'openai-completions'),
        }

    # é™çº§ generic
    if 'generic' in providers:
        p = providers['generic']
        model_id = p.get('models', [{}])[0].get('id', '') if p.get('models') else ''
        return {
            'base_url': p.get('baseUrl', ''),
            'api_key': p.get('apiKey', ''),
            'model': model_id,
            'api_type': p.get('api', 'openai-completions'),
        }

    return None


def list_templates():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡æ¿"""
    templates = []
    if not TEMPLATES_DIR.exists():
        return templates
    for f in sorted(TEMPLATES_DIR.glob('*.json')):
        try:
            with open(f, 'r', encoding='utf-8') as fh:
                t = json.load(fh)
            templates.append({
                'id': t.get('id', f.stem),
                'name': t.get('name', f.stem),
                'description': t.get('description', ''),
            })
        except Exception:
            pass
    return templates


def load_template(style):
    """åŠ è½½æŒ‡å®šé£æ ¼çš„æ¨¡æ¿"""
    path = TEMPLATES_DIR / f'{style}.json'
    if not path.exists():
        # å°è¯•æŒ‰ id åŒ¹é…
        for f in TEMPLATES_DIR.glob('*.json'):
            try:
                with open(f, 'r', encoding='utf-8') as fh:
                    t = json.load(fh)
                if t.get('id') == style:
                    return t
            except Exception:
                pass
        return None
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def call_llm(system_prompt, user_prompt, llm_cfg):
    """è°ƒç”¨ LLM API ç”Ÿæˆå†…å®¹ï¼ˆæ”¯æŒä»£ç†å’Œé€Ÿç‡é™åˆ¶é‡è¯•ï¼‰"""
    import urllib.request
    import urllib.error

    api_type = llm_cfg.get('api_type', 'openai-completions')
    base_url = llm_cfg['base_url'].rstrip('/')
    api_key = llm_cfg['api_key']
    model = llm_cfg['model']
    proxy = llm_cfg.get('proxy', '')

    if 'anthropic' in api_type:
        url = f"{base_url}/v1/messages"
        headers = {
            'Content-Type': 'application/json',
            'x-api-key': api_key,
            'anthropic-version': '2023-06-01',
        }
        body = {
            'model': model,
            'max_tokens': 4096,
            'system': system_prompt,
            'messages': [{'role': 'user', 'content': user_prompt}],
        }
    else:
        url = f"{base_url}/chat/completions"
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}',
        }
        body = {
            'model': model,
            'messages': [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt},
            ],
            'temperature': 0.8,
            'max_tokens': 16384,
        }

    data = json.dumps(body).encode('utf-8')

    # æ„å»º openerï¼ˆæ”¯æŒä»£ç†ï¼‰
    if proxy:
        proxy_handler = urllib.request.ProxyHandler({'https': proxy, 'http': proxy})
        opener = urllib.request.build_opener(proxy_handler)
    else:
        opener = urllib.request.build_opener()

    # é‡è¯•é€»è¾‘ï¼ˆGemini å…è´¹ tier æœ‰é€Ÿç‡é™åˆ¶ï¼‰
    max_retries = 3
    for attempt in range(max_retries):
        req = urllib.request.Request(url, data=data, headers=headers, method='POST')
        try:
            with opener.open(req, timeout=90) as resp:
                result = json.loads(resp.read().decode('utf-8'))
            break
        except urllib.error.HTTPError as e:
            err_body = e.read().decode('utf-8', errors='replace')
            if e.code == 429 and attempt < max_retries - 1:
                wait = (attempt + 1) * 15
                print(f"[LLM] é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait}s åé‡è¯• ({attempt+1}/{max_retries})...", file=sys.stderr)
                import time
                time.sleep(wait)
                continue
            raise RuntimeError(f"LLM API é”™è¯¯ ({e.code}): {err_body}")

    # æå–æ–‡æœ¬
    if 'anthropic' in api_type:
        text = result.get('content', [{}])[0].get('text', '')
    else:
        text = result.get('choices', [{}])[0].get('message', {}).get('content', '')

    return text


def _call_llm(prompt, max_tokens=4096):
    """ç®€æ˜“ LLM è°ƒç”¨ï¼ˆä¾› comments.py ç­‰æ¨¡å—ä½¿ç”¨ï¼‰"""
    llm_cfg = get_llm_config()
    if not llm_cfg:
        raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„ LLM é…ç½®")
    # ä¸´æ—¶è¦†ç›– max_tokens
    orig_call = call_llm
    import copy
    cfg = copy.copy(llm_cfg)
    return orig_call('', prompt, cfg)


def extract_json(text):
    """ä» LLM è¾“å‡ºä¸­æå– JSONï¼ˆå…¼å®¹ markdown code block åŒ…è£¹ï¼‰"""
    # å»æ‰ markdown code block
    text = re.sub(r'^```(?:json)?\s*\n?', '', text.strip())
    text = re.sub(r'\n?```\s*$', '', text.strip())

    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # å°è¯•æ‰¾ç¬¬ä¸€ä¸ª { ... } å—
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        try:
            return json.loads(match.group())
        except json.JSONDecodeError:
            pass

    raise ValueError(f"æ— æ³•ä» LLM è¾“å‡ºä¸­æå– JSON:\n{text[:500]}")


def generate_content(topic, style='default', extra_instructions=''):
    """
    æ ¹æ®ä¸»é¢˜å’Œé£æ ¼ç”Ÿæˆå°çº¢ä¹¦å†…å®¹

    Args:
        topic: ä¸»é¢˜/å…³é”®è¯
        style: æ–‡æ¡ˆé£æ ¼ (default/review/tutorial/daily/listicle/story/debate/comparison)
        extra_instructions: é¢å¤–æŒ‡ä»¤

    Returns:
        dict: {title, content, tags, call_to_action, style, topic}
    """
    # åŠ è½½ LLM é…ç½®
    llm_cfg = get_llm_config()
    if not llm_cfg:
        raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„ LLM é…ç½®ï¼Œè¯·æ£€æŸ¥ ~/.openclaw/openclaw.json")

    # åŠ è½½æ¨¡æ¿
    template = load_template(style)
    if not template:
        print(f"[å†…å®¹ç”Ÿæˆ] æœªæ‰¾åˆ°æ¨¡æ¿ '{style}'ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿", file=sys.stderr)
        template = load_template('default')
    if not template:
        raise RuntimeError(f"æ¨¡æ¿åŠ è½½å¤±è´¥: {style}")

    system_prompt = template.get('system', 'ä½ æ˜¯ä¸€ä½èµ„æ·±å°çº¢ä¹¦å†…å®¹åˆ›ä½œè€…ã€‚')
    user_prompt = template['user_template'].replace('{topic}', topic)

    if extra_instructions:
        user_prompt += f"\n\né¢å¤–è¦æ±‚ï¼š{extra_instructions}"

    print(f"[å†…å®¹ç”Ÿæˆ] ä¸»é¢˜: {topic} | é£æ ¼: {template['name']} | æ¨¡å‹: {llm_cfg['model']}", file=sys.stderr)

    # è°ƒç”¨ LLM
    raw_text = call_llm(system_prompt, user_prompt, llm_cfg)

    # è§£æ JSON
    result = extract_json(raw_text)

    # æ ‡å‡†åŒ–è¾“å‡º
    output = {
        'title': result.get('title', ''),
        'content': result.get('content', result.get('full_content', '')),
        'tags': result.get('tags', result.get('hashtags', [])),
        'call_to_action': result.get('call_to_action', ''),
        'style': template.get('id', style),
        'topic': topic,
        'generated_at': datetime.now().isoformat(),
        'model': llm_cfg['model'],
    }

    # æ¸…ç†æ ‡ç­¾æ ¼å¼ï¼ˆç¡®ä¿ä¸å¸¦ #ï¼‰
    output['tags'] = [t.lstrip('#').strip() for t in output['tags'] if t.strip()]

    # ç¡¬æ€§å­—æ•°é™åˆ¶ï¼ˆå°çº¢ä¹¦æ ‡é¢˜20å­—ã€æ­£æ–‡1000å­—ï¼‰
    if len(output['title']) > 20:
        # åœ¨20å­—å†…æ‰¾æœ€åä¸€ä¸ªæ ‡ç‚¹æˆ–ç©ºæ ¼æˆªæ–­ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
        t = output['title'][:20]
        for i in range(19, 14, -1):
            if t[i] in 'ï¼Œã€‚ï¼ï¼Ÿã€Â·~â€¦â€”|,!? ':
                t = t[:i]
                break
        output['title'] = t
        print(f"[å†…å®¹ç”Ÿæˆ] æ ‡é¢˜è¶…é•¿å·²æˆªæ–­: {output['title']}", file=sys.stderr)

    # åˆè§„ï¼šæ­£æ–‡æœ«å°¾è¿½åŠ  AI è¾…åŠ©åˆ›ä½œå£°æ˜
    if output['content'] and not output['content'].rstrip().endswith('AIè¾…åŠ©åˆ›ä½œ'):
        output['content'] = output['content'].rstrip() + '\n\nğŸ“ æœ¬æ–‡ç”± AI è¾…åŠ©åˆ›ä½œ'

    # æ­£æ–‡è¶…é•¿å¤„ç†ï¼šæ‹†åˆ†ä¸ºç¼–è¾‘å™¨æ–‡æœ¬ + æº¢å‡ºæ–‡æœ¬ï¼ˆæº¢å‡ºéƒ¨åˆ†å°†ç”Ÿæˆæ–‡å­—å›¾ç‰‡ï¼‰
    full_content = output['content']
    MAX_EDITOR = 950  # ç¼–è¾‘å™¨å®‰å…¨ä¸Šé™ï¼ˆç•™ä½™é‡ç»™å£°æ˜ï¼‰

    if len(full_content) > MAX_EDITOR:
        # åœ¨ MAX_EDITOR å­—å†…æ‰¾æ®µè½è¾¹ç•Œæˆªæ–­
        cut = full_content[:MAX_EDITOR]
        last_para = cut.rfind('\n\n')
        if last_para > 500:
            cut_pos = last_para
        else:
            last_nl = cut.rfind('\n')
            cut_pos = last_nl if last_nl > 500 else MAX_EDITOR

        editor_text = full_content[:cut_pos].rstrip()
        overflow_text = full_content[cut_pos:].strip()

        # ç¼–è¾‘å™¨æ–‡æœ¬æœ«å°¾åŠ "ğŸ‘‰ æ›´å¤šå†…å®¹è§å›¾ç‰‡"å¼•å¯¼ + å£°æ˜
        if not editor_text.endswith('AIè¾…åŠ©åˆ›ä½œ'):
            editor_text = editor_text + '\n\nğŸ‘‰ æ›´å¤šå†…å®¹è§å›¾ç‰‡\n\nğŸ“ æœ¬æ–‡ç”± AI è¾…åŠ©åˆ›ä½œ'

        # æº¢å‡ºæ–‡æœ¬å»æ‰é‡å¤çš„å£°æ˜ï¼ˆä¼šåœ¨å›¾ç‰‡æ°´å°é‡Œä½“ç°ï¼‰
        overflow_text = overflow_text.replace('ğŸ“ æœ¬æ–‡ç”± AI è¾…åŠ©åˆ›ä½œ', '').strip()

        output['content'] = editor_text
        output['overflow_text'] = overflow_text
        print(f"[å†…å®¹ç”Ÿæˆ] æ­£æ–‡è¶…é•¿: ç¼–è¾‘å™¨ {len(editor_text)} å­— + æº¢å‡º {len(overflow_text)} å­—ï¼ˆå°†ç”Ÿæˆæ–‡å­—å›¾ç‰‡ï¼‰", file=sys.stderr)
    else:
        output['overflow_text'] = ''

    return output


def save_content(content_data, filename=None):
    """ä¿å­˜ç”Ÿæˆçš„å†…å®¹åˆ° JSON æ–‡ä»¶"""
    if not filename:
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"gen_{ts}.json"
    path = CONTENT_DIR / filename
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(content_data, f, ensure_ascii=False, indent=2)
    return str(path)


# â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_generate(args):
    """ç”Ÿæˆå†…å®¹"""
    try:
        result = generate_content(
            topic=args.topic,
            style=args.style,
            extra_instructions=args.extra or '',
        )

        # ä¿å­˜åˆ°æ–‡ä»¶
        if args.save:
            path = save_content(result)
            result['saved_to'] = path
            print(f"[å†…å®¹ç”Ÿæˆ] å·²ä¿å­˜åˆ°: {path}", file=sys.stderr)

        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        print(json.dumps({
            'success': False,
            'error': str(e),
        }, ensure_ascii=False, indent=2))
        sys.exit(1)


def cmd_list_styles(args):
    """åˆ—å‡ºå¯ç”¨é£æ ¼"""
    templates = list_templates()
    if not templates:
        print("æš‚æ— å¯ç”¨æ¨¡æ¿")
        return
    print(json.dumps(templates, ensure_ascii=False, indent=2))


def main():
    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦ AI å†…å®¹ç”Ÿæˆå™¨')
    sub = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # generate
    p_gen = sub.add_parser('generate', help='ç”Ÿæˆå°çº¢ä¹¦å†…å®¹')
    p_gen.add_argument('topic', help='ä¸»é¢˜/å…³é”®è¯')
    p_gen.add_argument('--style', '-s', default='default',
                       help='æ–‡æ¡ˆé£æ ¼: default/review/tutorial/daily/listicle/story/debate/comparison')
    p_gen.add_argument('--extra', '-e', help='é¢å¤–æŒ‡ä»¤')
    p_gen.add_argument('--save', action='store_true', help='ä¿å­˜åˆ°æ–‡ä»¶')

    # styles
    sub.add_parser('styles', help='åˆ—å‡ºå¯ç”¨æ–‡æ¡ˆé£æ ¼')

    args = parser.parse_args()

    if args.command == 'generate':
        cmd_generate(args)
    elif args.command == 'styles':
        cmd_list_styles(args)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()
