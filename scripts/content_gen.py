#!/usr/bin/env python3
"""
å°çº¢ä¹¦ AI å†…å®¹ç”Ÿæˆå™¨
ä½¿ç”¨ OpenClaw å·²é…ç½®çš„ LLMï¼ˆç™¾ç‚¼ Qwenï¼‰ç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„æ ‡é¢˜ã€æ­£æ–‡å’Œæ ‡ç­¾ã€‚
æ”¯æŒå¤šç§æ–‡æ¡ˆé£æ ¼æ¨¡æ¿ã€‚
"""

import os
import sys
import json
import argparse
import re
from pathlib import Path
from datetime import datetime

# è·¯å¾„å¸¸é‡
SKILL_DIR = Path(__file__).parent.parent
TEMPLATES_DIR = SKILL_DIR / 'templates'
CONTENT_DIR = SKILL_DIR / 'content'
OPENCLAW_CONFIG = Path.home() / '.openclaw' / 'openclaw.json'

CONTENT_DIR.mkdir(exist_ok=True)


def load_config():
    """åŠ è½½ OpenClaw é…ç½®ï¼Œè·å– API ä¿¡æ¯"""
    try:
        with open(OPENCLAW_CONFIG, 'r') as f:
            return json.load(f)
    except Exception:
        return {}


def get_llm_config():
    """è·å– LLM API é…ç½®ï¼ˆä¼˜å…ˆ Gemini å…è´¹ APIï¼Œé™çº§ç™¾ç‚¼ï¼‰"""
    cfg = load_config()
    providers = cfg.get('models', {}).get('providers', {})

    # ä¼˜å…ˆç”¨ Geminiï¼ˆå…è´¹ï¼‰
    gemini_key = None
    try:
        gemini_key = cfg['skills']['entries']['nano-banana-pro']['apiKey']
    except (KeyError, TypeError):
        pass
    if not gemini_key:
        gemini_key = os.environ.get('GEMINI_API_KEY', '')

    if gemini_key:
        return {
            'base_url': 'https://generativelanguage.googleapis.com/v1beta/openai',
            'api_key': gemini_key,
            'model': 'gemini-2.5-flash',
            'api_type': 'openai-completions',
            'proxy': 'http://127.0.0.1:7897',
        }

    # é™çº§ç™¾ç‚¼
    if 'bailian' in providers:
        p = providers['bailian']
        model_id = p.get('models', [{}])[0].get('id', 'qwen-plus') if p.get('models') else 'qwen-plus'
        return {
            'base_url': p.get('baseUrl', ''),
            'api_key': p.get('apiKey', ''),
            'model': model_id,
            'api_type': p.get('api', 'openai-completions'),
        }

    # é™çº§ generic
    if 'generic' in providers:
        p = providers['generic']
        model_id = p.get('models', [{}])[0].get('id', '') if p.get('models') else ''
        return {
            'base_url': p.get('baseUrl', ''),
            'api_key': p.get('apiKey', ''),
            'model': model_id,
            'api_type': p.get('api', 'openai-completions'),
        }

    return None


def list_templates():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡æ¿"""
    templates = []
    if not TEMPLATES_DIR.exists():
        return templates
    for f in sorted(TEMPLATES_DIR.glob('*.json')):
        try:
            with open(f, 'r', encoding='utf-8') as fh:
                t = json.load(fh)
            templates.append({
                'id': t.get('id', f.stem),
                'name': t.get('name', f.stem),
                'description': t.get('description', ''),
            })
        except Exception:
            pass
    return templates


def load_template(style):
    """åŠ è½½æŒ‡å®šé£æ ¼çš„æ¨¡æ¿"""
    path = TEMPLATES_DIR / f'{style}.json'
    if not path.exists():
        # å°è¯•æŒ‰ id åŒ¹é…
        for f in TEMPLATES_DIR.glob('*.json'):
            try:
                with open(f, 'r', encoding='utf-8') as fh:
                    t = json.load(fh)
                if t.get('id') == style:
                    return t
            except Exception:
                pass
        return None
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)


def call_llm(system_prompt, user_prompt, llm_cfg):
    """è°ƒç”¨ LLM API ç”Ÿæˆå†…å®¹ï¼ˆæ”¯æŒä»£ç†å’Œé€Ÿç‡é™åˆ¶é‡è¯•ï¼‰"""
    import urllib.request
    import urllib.error

    api_type = llm_cfg.get('api_type', 'openai-completions')
    base_url = llm_cfg['base_url'].rstrip('/')
    api_key = llm_cfg['api_key']
    model = llm_cfg['model']
    proxy = llm_cfg.get('proxy', '')

    if 'anthropic' in api_type:
        url = f"{base_url}/v1/messages"
        headers = {
            'Content-Type': 'application/json',
            'x-api-key': api_key,
            'anthropic-version': '2023-06-01',
        }
        body = {
            'model': model,
            'max_tokens': 4096,
            'system': system_prompt,
            'messages': [{'role': 'user', 'content': user_prompt}],
        }
    else:
        url = f"{base_url}/chat/completions"
        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {api_key}',
        }
        body = {
            'model': model,
            'messages': [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt},
            ],
            'temperature': 0.8,
            'max_tokens': 16384,
        }

    data = json.dumps(body).encode('utf-8')

    # æ„å»º openerï¼ˆæ”¯æŒä»£ç†ï¼‰
    if proxy:
        proxy_handler = urllib.request.ProxyHandler({'https': proxy, 'http': proxy})
        opener = urllib.request.build_opener(proxy_handler)
    else:
        opener = urllib.request.build_opener()

    # é‡è¯•é€»è¾‘ï¼ˆGemini å…è´¹ tier æœ‰é€Ÿç‡é™åˆ¶ï¼‰
    max_retries = 3
    for attempt in range(max_retries):
        req = urllib.request.Request(url, data=data, headers=headers, method='POST')
        try:
            with opener.open(req, timeout=90) as resp:
                result = json.loads(resp.read().decode('utf-8'))
            break
        except urllib.error.HTTPError as e:
            err_body = e.read().decode('utf-8', errors='replace')
            if e.code == 429 and attempt < max_retries - 1:
                wait = (attempt + 1) * 15
                print(f"[LLM] é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait}s åé‡è¯• ({attempt+1}/{max_retries})...", file=sys.stderr)
                import time
                time.sleep(wait)
                continue
            raise RuntimeError(f"LLM API é”™è¯¯ ({e.code}): {err_body}")

    # æå–æ–‡æœ¬
    if 'anthropic' in api_type:
        text = result.get('content', [{}])[0].get('text', '')
    else:
        text = result.get('choices', [{}])[0].get('message', {}).get('content', '')

    return text


def _call_llm(prompt, max_tokens=4096):
    """ç®€æ˜“ LLM è°ƒç”¨ï¼ˆä¾› comments.py ç­‰æ¨¡å—ä½¿ç”¨ï¼‰"""
    llm_cfg = get_llm_config()
    if not llm_cfg:
        raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„ LLM é…ç½®")
    # ä¸´æ—¶è¦†ç›– max_tokens
    orig_call = call_llm
    import copy
    cfg = copy.copy(llm_cfg)
    return orig_call('', prompt, cfg)


def extract_json(text):
    """ä» LLM è¾“å‡ºä¸­æå– JSONï¼ˆå…¼å®¹ markdown code block åŒ…è£¹ï¼‰"""
    # å»æ‰ markdown code block
    text = re.sub(r'^```(?:json)?\s*\n?', '', text.strip())
    text = re.sub(r'\n?```\s*$', '', text.strip())

    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # å°è¯•æ‰¾ç¬¬ä¸€ä¸ª { ... } å—
    match = re.search(r'\{[\s\S]*\}', text)
    if match:
        try:
            return json.loads(match.group())
        except json.JSONDecodeError:
            pass

    raise ValueError(f"æ— æ³•ä» LLM è¾“å‡ºä¸­æå– JSON:\n{text[:500]}")


def generate_content(topic, style='default', extra_instructions=''):
    """
    æ ¹æ®ä¸»é¢˜å’Œé£æ ¼ç”Ÿæˆå°çº¢ä¹¦å†…å®¹

    Args:
        topic: ä¸»é¢˜/å…³é”®è¯
        style: æ–‡æ¡ˆé£æ ¼ (default/review/tutorial/daily/listicle/story/debate/comparison)
        extra_instructions: é¢å¤–æŒ‡ä»¤

    Returns:
        dict: {title, content, tags, call_to_action, style, topic}
    """
    # åŠ è½½ LLM é…ç½®
    llm_cfg = get_llm_config()
    if not llm_cfg:
        raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨çš„ LLM é…ç½®ï¼Œè¯·æ£€æŸ¥ ~/.openclaw/openclaw.json")

    # åŠ è½½æ¨¡æ¿
    template = load_template(style)
    if not template:
        print(f"[å†…å®¹ç”Ÿæˆ] æœªæ‰¾åˆ°æ¨¡æ¿ '{style}'ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿", file=sys.stderr)
        template = load_template('default')
    if not template:
        raise RuntimeError(f"æ¨¡æ¿åŠ è½½å¤±è´¥: {style}")

    system_prompt = template.get('system', 'ä½ æ˜¯ä¸€ä½èµ„æ·±å°çº¢ä¹¦å†…å®¹åˆ›ä½œè€…ã€‚')
    # é€šç”¨çº¦æŸ
    system_prompt += '\n\né‡è¦çº¦æŸï¼š\n1. æ­£æ–‡ä¸­ç»å¯¹ä¸è¦å‡ºç°ä»»ä½•ä»£ç ç‰‡æ®µã€ä»£ç å—æˆ–æŠ€æœ¯å‘½ä»¤ã€‚è®²æ–¹æ³•ã€è®²æ€è·¯å³å¯ï¼Œç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šã€‚\n2. ä¸è¦ä½¿ç”¨ä»»ä½• Markdown æ ¼å¼ï¼ˆå¦‚ #ã€##ã€**åŠ ç²—**ã€*æ–œä½“*ã€- åˆ—è¡¨ç¬¦å·ç­‰ï¼‰ã€‚ç”¨ emoji å’Œæ¢è¡Œæ¥ç»„ç»‡æ’ç‰ˆï¼Œç¬¦åˆå°çº¢ä¹¦çš„é˜…è¯»ä¹ æƒ¯ã€‚'
    user_prompt = template['user_template'].replace('{topic}', topic)

    if extra_instructions:
        user_prompt += f"\n\né¢å¤–è¦æ±‚ï¼š{extra_instructions}"

    print(f"[å†…å®¹ç”Ÿæˆ] ä¸»é¢˜: {topic} | é£æ ¼: {template['name']} | æ¨¡å‹: {llm_cfg['model']}", file=sys.stderr)

    # è°ƒç”¨ LLM
    raw_text = call_llm(system_prompt, user_prompt, llm_cfg)

    # è§£æ JSON
    result = extract_json(raw_text)

    # æ ‡å‡†åŒ–è¾“å‡º
    output = {
        'title': result.get('title', ''),
        'content': result.get('content', result.get('full_content', '')),
        'tags': result.get('tags', result.get('hashtags', [])),
        'call_to_action': result.get('call_to_action', ''),
        'style': template.get('id', style),
        'topic': topic,
        'generated_at': datetime.now().isoformat(),
        'model': llm_cfg['model'],
    }

    # æ¸…ç†æ ‡ç­¾æ ¼å¼ï¼ˆç¡®ä¿ä¸å¸¦ #ï¼‰
    output['tags'] = [t.lstrip('#').strip() for t in output['tags'] if t.strip()]

    # ç¡¬æ€§å­—æ•°é™åˆ¶ï¼ˆå°çº¢ä¹¦æ ‡é¢˜20å­—ã€æ­£æ–‡1000å­—ï¼‰
    if len(output['title']) > 20:
        # åœ¨20å­—å†…æ‰¾æœ€åä¸€ä¸ªæ ‡ç‚¹æˆ–ç©ºæ ¼æˆªæ–­ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
        t = output['title'][:20]
        for i in range(19, 14, -1):
            if t[i] in 'ï¼Œã€‚ï¼ï¼Ÿã€Â·~â€¦â€”|,!? ':
                t = t[:i]
                break
        output['title'] = t
        print(f"[å†…å®¹ç”Ÿæˆ] æ ‡é¢˜è¶…é•¿å·²æˆªæ–­: {output['title']}", file=sys.stderr)

    # æ­£æ–‡è¶…é•¿å¤„ç†ï¼šè¶…è¿‡ç¼–è¾‘å™¨é™åˆ¶æ—¶ï¼Œå…¨éƒ¨å†…å®¹åšæˆæ–‡å­—å›¾ç‰‡
    full_content = output['content']
    MAX_EDITOR = 950  # ç¼–è¾‘å™¨å®‰å…¨ä¸Šé™

    if len(full_content) > MAX_EDITOR:
        # å…¨éƒ¨å†…å®¹è½¬ä¸ºå›¾ç‰‡æ–‡æœ¬ï¼Œç¼–è¾‘å™¨åªæ”¾å¼•å¯¼è¯­
        overflow_text = full_content.rstrip()
        # å»æ‰å¯èƒ½å·²æœ‰çš„å£°æ˜ï¼ˆå›¾ç‰‡æ°´å°ä¼šä½“ç°ï¼‰
        overflow_text = overflow_text.replace('ğŸ“ æœ¬æ–‡ç”± AI è¾…åŠ©åˆ›ä½œ', '').strip()

        editor_text = 'ğŸ‘‰ å®Œæ•´å†…å®¹è§å›¾ç‰‡ï¼Œå·¦æ»‘æŸ¥çœ‹å…¨æ–‡\n\nğŸ“ æœ¬æ–‡ç”± AI è¾…åŠ©åˆ›ä½œ'

        output['content'] = editor_text
        output['overflow_text'] = overflow_text
        print(f"[å†…å®¹ç”Ÿæˆ] æ­£æ–‡è¶…é•¿({len(full_content)}å­—): å…¨éƒ¨è½¬ä¸ºæ–‡å­—å›¾ç‰‡", file=sys.stderr)
    else:
        # æ­£å¸¸é•¿åº¦ï¼Œè¿½åŠ  AI å£°æ˜
        if full_content and not full_content.rstrip().endswith('AIè¾…åŠ©åˆ›ä½œ'):
            output['content'] = full_content.rstrip() + '\n\nğŸ“ æœ¬æ–‡ç”± AI è¾…åŠ©åˆ›ä½œ'
        output['overflow_text'] = ''

    return output


def save_content(content_data, filename=None):
    """ä¿å­˜ç”Ÿæˆçš„å†…å®¹åˆ° JSON æ–‡ä»¶"""
    if not filename:
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"gen_{ts}.json"
    path = CONTENT_DIR / filename
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(content_data, f, ensure_ascii=False, indent=2)
    return str(path)


# â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def cmd_generate(args):
    """ç”Ÿæˆå†…å®¹"""
    try:
        result = generate_content(
            topic=args.topic,
            style=args.style,
            extra_instructions=args.extra or '',
        )

        # ä¿å­˜åˆ°æ–‡ä»¶
        if args.save:
            path = save_content(result)
            result['saved_to'] = path
            print(f"[å†…å®¹ç”Ÿæˆ] å·²ä¿å­˜åˆ°: {path}", file=sys.stderr)

        print(json.dumps(result, ensure_ascii=False, indent=2))

    except Exception as e:
        print(json.dumps({
            'success': False,
            'error': str(e),
        }, ensure_ascii=False, indent=2))
        sys.exit(1)


def cmd_list_styles(args):
    """åˆ—å‡ºå¯ç”¨é£æ ¼"""
    templates = list_templates()
    if not templates:
        print("æš‚æ— å¯ç”¨æ¨¡æ¿")
        return
    print(json.dumps(templates, ensure_ascii=False, indent=2))


def main():
    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦ AI å†…å®¹ç”Ÿæˆå™¨')
    sub = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # generate
    p_gen = sub.add_parser('generate', help='ç”Ÿæˆå°çº¢ä¹¦å†…å®¹')
    p_gen.add_argument('topic', help='ä¸»é¢˜/å…³é”®è¯')
    p_gen.add_argument('--style', '-s', default='default',
                       help='æ–‡æ¡ˆé£æ ¼: default/review/tutorial/daily/listicle/story/debate/comparison')
    p_gen.add_argument('--extra', '-e', help='é¢å¤–æŒ‡ä»¤')
    p_gen.add_argument('--save', action='store_true', help='ä¿å­˜åˆ°æ–‡ä»¶')

    # styles
    sub.add_parser('styles', help='åˆ—å‡ºå¯ç”¨æ–‡æ¡ˆé£æ ¼')

    args = parser.parse_args()

    if args.command == 'generate':
        cmd_generate(args)
    elif args.command == 'styles':
        cmd_list_styles(args)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()
