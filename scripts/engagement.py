#!/usr/bin/env python3
"""
ç¬”è®°æ•°æ®æŠ“å–æ¨¡å—
ä»å°çº¢ä¹¦åˆ›ä½œè€…ä¸­å¿ƒæŠ“å–ç¬”è®°çš„é˜…è¯»ã€ç‚¹èµã€æ”¶è—ã€è¯„è®ºç­‰äº’åŠ¨æ•°æ®
"""

import json
import time
import logging
import sys
from pathlib import Path
from datetime import datetime

log = logging.getLogger(__name__)

SKILL_DIR = Path(__file__).parent.parent
DATA_DIR = SKILL_DIR / 'data'
DATA_DIR.mkdir(exist_ok=True)

ENGAGEMENT_DB = DATA_DIR / 'engagement.json'
XHS_CONTENT_MANAGE = 'https://creator.xiaohongshu.com/publish/manage'


def _load_engagement_db():
    """åŠ è½½äº’åŠ¨æ•°æ®åº“"""
    if ENGAGEMENT_DB.exists():
        try:
            with open(ENGAGEMENT_DB, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
    return {"notes": {}, "snapshots": []}


def _save_engagement_db(db):
    """ä¿å­˜äº’åŠ¨æ•°æ®åº“"""
    with open(ENGAGEMENT_DB, 'w', encoding='utf-8') as f:
        json.dump(db, f, ensure_ascii=False, indent=2)


def fetch_note_engagement(page, limit=20):
    """
    ä»åˆ›ä½œè€…ä¸­å¿ƒã€Œå†…å®¹ç®¡ç†ã€é¡µæŠ“å–ç¬”è®°äº’åŠ¨æ•°æ®
    è¿”å›: [{"title", "status", "views", "likes", "collects", "comments", "shares", "publish_time"}]
    """
    log.info(f'æ­£åœ¨æŠ“å–ç¬”è®°äº’åŠ¨æ•°æ®ï¼ˆæœ€å¤š {limit} æ¡ï¼‰...')
    page.goto(XHS_CONTENT_MANAGE, wait_until='domcontentloaded', timeout=15000)
    time.sleep(3)

    notes = []
    seen_titles = set()

    max_scrolls = min(limit // 5 + 2, 15)
    for scroll_i in range(max_scrolls):
        # å°è¯•å¤šç§é€‰æ‹©å™¨åŒ¹é…ç¬”è®°åˆ—è¡¨é¡¹
        rows = page.locator(
            '.note-item, [class*="note-item"], [class*="NoteItem"], '
            'table tbody tr, .content-item, [class*="content-item"], '
            '[class*="ManageNote"], .manage-note-item'
        ).all()

        if not rows:
            # å¤‡ç”¨ï¼šå°è¯•è¡¨æ ¼è¡Œ
            rows = page.locator('.ant-table-row, [class*="table"] tr').all()

        for row in rows:
            try:
                # æå–æ ‡é¢˜
                title = ''
                for sel in ['[class*="title"]', '.note-title', 'a', '[class*="name"]']:
                    try:
                        el = row.locator(sel).first
                        t = el.inner_text(timeout=1000).strip()
                        if t and len(t) > 2:
                            title = t[:50]
                            break
                    except Exception:
                        continue

                if not title or title in seen_titles:
                    continue
                seen_titles.add(title)

                # æå–æ•°å€¼æ•°æ® â€” å°è¯•ä»è¡Œå†…æ‰€æœ‰æ•°å­—å…ƒç´ ä¸­æå–
                numbers = []
                try:
                    # è·å–è¡Œå†…æ‰€æœ‰æ–‡æœ¬ï¼Œæå–æ•°å­—
                    row_text = row.inner_text(timeout=2000)
                    import re
                    # åŒ¹é…æ•°å­—ï¼ˆåŒ…æ‹¬å¸¦ä¸‡/wçš„ï¼‰
                    raw_nums = re.findall(r'([\d.]+[ä¸‡w]?)', row_text)
                    for n in raw_nums:
                        numbers.append(_parse_number(n))
                except Exception:
                    pass

                # æå–çŠ¶æ€
                status = 'å·²å‘å¸ƒ'
                try:
                    for kw in ['å®¡æ ¸ä¸­', 'æœªé€šè¿‡', 'å·²éšè—', 'è‰ç¨¿', 'å·²å‘å¸ƒ', 'å…¬å¼€']:
                        if kw in row_text:
                            status = kw
                            break
                except Exception:
                    pass

                # å°è¯•ä»ç‰¹å®š class æå–å„é¡¹æ•°æ®
                data = {
                    "title": title,
                    "status": status,
                    "views": 0,
                    "likes": 0,
                    "collects": 0,
                    "comments": 0,
                    "shares": 0,
                }

                # å°è¯•æŒ‰åˆ—ååŒ¹é…
                for field, keywords in [
                    ('views', ['é˜…è¯»', 'è§‚çœ‹', 'æµè§ˆ', 'view', 'read', 'æ›å…‰']),
                    ('likes', ['ç‚¹èµ', 'èµ', 'like', 'â¤']),
                    ('collects', ['æ”¶è—', 'collect', 'star', 'â­']),
                    ('comments', ['è¯„è®º', 'comment', 'ğŸ’¬']),
                    ('shares', ['åˆ†äº«', 'share', 'è½¬å‘']),
                ]:
                    for kw in keywords:
                        try:
                            el = row.locator(f'[class*="{kw}"], [title*="{kw}"]').first
                            val = el.inner_text(timeout=500).strip()
                            data[field] = _parse_number(val)
                            break
                        except Exception:
                            continue

                # å¦‚æœç‰¹å®šåŒ¹é…æ²¡æ‹¿åˆ°æ•°æ®ï¼Œç”¨ä½ç½®æ¨æ–­
                # åˆ›ä½œè€…ä¸­å¿ƒé€šå¸¸åˆ—é¡ºåºï¼šæ ‡é¢˜ | çŠ¶æ€ | é˜…è¯» | ç‚¹èµ | æ”¶è— | è¯„è®º | åˆ†äº«
                if all(data[k] == 0 for k in ['views', 'likes', 'collects', 'comments']) and len(numbers) >= 3:
                    fields = ['views', 'likes', 'collects', 'comments', 'shares']
                    for i, num in enumerate(numbers[:len(fields)]):
                        data[fields[i]] = num

                notes.append(data)

                if len(notes) >= limit:
                    break
            except Exception as e:
                log.debug(f'è§£æç¬”è®°è¡Œå¤±è´¥: {e}')
                continue

        if len(notes) >= limit:
            break

        page.evaluate('window.scrollBy(0, 600)')
        time.sleep(1.5)

    log.info(f'å…±æŠ“å–åˆ° {len(notes)} æ¡ç¬”è®°æ•°æ®')

    # ä¿å­˜å¿«ç…§
    db = _load_engagement_db()
    snapshot = {
        "time": datetime.now().isoformat(),
        "count": len(notes),
        "notes": notes,
    }
    db['snapshots'].append(snapshot)
    # åªä¿ç•™æœ€è¿‘ 60 ä¸ªå¿«ç…§
    if len(db['snapshots']) > 60:
        db['snapshots'] = db['snapshots'][-60:]

    # æ›´æ–°æ¯ç¯‡ç¬”è®°çš„æœ€æ–°æ•°æ®
    for note in notes:
        db['notes'][note['title']] = {
            **note,
            "last_updated": datetime.now().isoformat(),
        }
    _save_engagement_db(db)

    return notes


def _parse_number(text):
    """è§£ææ•°å­—æ–‡æœ¬ï¼Œæ”¯æŒ '1.2ä¸‡'ã€'1.2w' ç­‰æ ¼å¼"""
    if not text:
        return 0
    text = text.strip()
    import re
    m = re.match(r'^([\d.]+)\s*[ä¸‡w]', text)
    if m:
        return int(float(m.group(1)) * 10000)
    m = re.match(r'^[\d.]+', text)
    if m:
        try:
            return int(float(m.group()))
        except ValueError:
            return 0
    return 0


def generate_daily_report(include_engagement=True, page=None):
    """
    ç”Ÿæˆæ¯æ—¥æ•°æ®æŠ¥å‘Š
    å¦‚æœæä¾› page ä¸” include_engagement=Trueï¼Œä¼šå…ˆæŠ“å–æœ€æ–°äº’åŠ¨æ•°æ®
    """
    sys.path.insert(0, str(Path(__file__).parent))
    from stats import load_reports, filter_by_date, summary

    # å‘å¸ƒç»Ÿè®¡
    reports = load_reports()
    today_reports = filter_by_date(reports, days=1)
    all_stats = summary(reports)
    today_stats = summary(today_reports)

    report = {
        "generated_at": datetime.now().isoformat(),
        "publish_stats": {
            "today": {
                "total": today_stats.get('total', 0),
                "success": today_stats.get('success', 0),
                "failed": today_stats.get('failed', 0),
            },
            "all_time": {
                "total": all_stats.get('total', 0),
                "success": all_stats.get('success', 0),
                "success_rate": all_stats.get('success_rate', '0%'),
            },
            "top_tags": all_stats.get('top_tags', [])[:5],
        },
        "engagement": None,
    }

    # äº’åŠ¨æ•°æ®
    if include_engagement and page:
        notes = fetch_note_engagement(page, limit=20)
        total_views = sum(n.get('views', 0) for n in notes)
        total_likes = sum(n.get('likes', 0) for n in notes)
        total_collects = sum(n.get('collects', 0) for n in notes)
        total_comments = sum(n.get('comments', 0) for n in notes)
        total_shares = sum(n.get('shares', 0) for n in notes)

        # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„ç¬”è®°
        best_note = max(notes, key=lambda n: n.get('likes', 0) + n.get('collects', 0), default=None)

        report['engagement'] = {
            "notes_count": len(notes),
            "total_views": total_views,
            "total_likes": total_likes,
            "total_collects": total_collects,
            "total_comments": total_comments,
            "total_shares": total_shares,
            "best_note": {
                "title": best_note['title'],
                "likes": best_note.get('likes', 0),
                "collects": best_note.get('collects', 0),
                "comments": best_note.get('comments', 0),
            } if best_note else None,
            "notes_detail": notes[:10],
        }
    elif include_engagement:
        # ä»ç¼“å­˜è¯»å–
        db = _load_engagement_db()
        if db.get('snapshots'):
            latest = db['snapshots'][-1]
            notes = latest.get('notes', [])
            total_views = sum(n.get('views', 0) for n in notes)
            total_likes = sum(n.get('likes', 0) for n in notes)
            total_collects = sum(n.get('collects', 0) for n in notes)
            total_comments = sum(n.get('comments', 0) for n in notes)
            total_shares = sum(n.get('shares', 0) for n in notes)
            best_note = max(notes, key=lambda n: n.get('likes', 0) + n.get('collects', 0), default=None)

            report['engagement'] = {
                "notes_count": len(notes),
                "total_views": total_views,
                "total_likes": total_likes,
                "total_collects": total_collects,
                "total_comments": total_comments,
                "total_shares": total_shares,
                "best_note": {
                    "title": best_note['title'],
                    "likes": best_note.get('likes', 0),
                    "collects": best_note.get('collects', 0),
                    "comments": best_note.get('comments', 0),
                } if best_note else None,
                "cached": True,
                "snapshot_time": latest.get('time', ''),
            }

    return report


def format_daily_report(report):
    """æ ¼å¼åŒ–æ¯æ—¥æŠ¥å‘Šä¸ºå¯è¯»æ–‡æœ¬"""
    lines = [
        f"ğŸ“Š å°çº¢ä¹¦æ¯æ—¥æ•°æ®æŠ¥å‘Š",
        f"ğŸ“… {report['generated_at'][:10]}",
        f"",
        f"ğŸ“ å‘å¸ƒç»Ÿè®¡",
        f"  ä»Šæ—¥å‘å¸ƒ: {report['publish_stats']['today']['total']} ç¯‡"
        f"ï¼ˆæˆåŠŸ {report['publish_stats']['today']['success']}ï¼Œ"
        f"å¤±è´¥ {report['publish_stats']['today']['failed']}ï¼‰",
        f"  ç´¯è®¡å‘å¸ƒ: {report['publish_stats']['all_time']['total']} ç¯‡"
        f"ï¼ˆæˆåŠŸç‡ {report['publish_stats']['all_time']['success_rate']}ï¼‰",
    ]

    tags = report['publish_stats'].get('top_tags', [])
    if tags:
        tag_str = ' '.join(f"#{t['tag']}" for t in tags[:5])
        lines.append(f"  çƒ­é—¨æ ‡ç­¾: {tag_str}")

    eng = report.get('engagement')
    if eng:
        lines.extend([
            f"",
            f"ğŸ’¬ äº’åŠ¨æ•°æ®ï¼ˆ{eng.get('notes_count', 0)} ç¯‡ç¬”è®°ï¼‰",
            f"  ğŸ‘€ é˜…è¯»: {eng.get('total_views', 0)}",
            f"  â¤ï¸ ç‚¹èµ: {eng.get('total_likes', 0)}",
            f"  â­ æ”¶è—: {eng.get('total_collects', 0)}",
            f"  ğŸ’¬ è¯„è®º: {eng.get('total_comments', 0)}",
            f"  ğŸ”— åˆ†äº«: {eng.get('total_shares', 0)}",
        ])
        best = eng.get('best_note')
        if best:
            lines.extend([
                f"",
                f"ğŸ† æœ€ä½³ç¬”è®°: {best['title']}",
                f"   â¤ï¸{best.get('likes', 0)} â­{best.get('collects', 0)} ğŸ’¬{best.get('comments', 0)}",
            ])
        if eng.get('cached'):
            lines.append(f"\n  âš ï¸ äº’åŠ¨æ•°æ®æ¥è‡ªç¼“å­˜ï¼ˆ{eng.get('snapshot_time', '')[:16]}ï¼‰")
    else:
        lines.extend(["", "ğŸ’¬ äº’åŠ¨æ•°æ®: æš‚æ— ï¼ˆéœ€è¦å…ˆæŠ“å–ï¼‰"])

    return "\n".join(lines)


def main():
    import argparse
    parser = argparse.ArgumentParser(description='ç¬”è®°äº’åŠ¨æ•°æ®')
    sub = parser.add_subparsers(dest='action')

    p_fetch = sub.add_parser('fetch', help='æŠ“å–ç¬”è®°äº’åŠ¨æ•°æ®')
    p_fetch.add_argument('--limit', type=int, default=20)
    p_fetch.add_argument('--headless', action='store_true')

    p_report = sub.add_parser('report', help='ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š')
    p_report.add_argument('--headless', action='store_true')
    p_report.add_argument('--no-engagement', action='store_true', help='ä¸æŠ“å–äº’åŠ¨æ•°æ®')
    p_report.add_argument('--json', action='store_true')

    p_cached = sub.add_parser('cached', help='æŸ¥çœ‹ç¼“å­˜çš„äº’åŠ¨æ•°æ®')

    args = parser.parse_args()

    if args.action == 'cached':
        db = _load_engagement_db()
        if db.get('snapshots'):
            latest = db['snapshots'][-1]
            print(json.dumps(latest, ensure_ascii=False, indent=2))
        else:
            print(json.dumps({"message": "æš‚æ— ç¼“å­˜æ•°æ®"}, ensure_ascii=False))
        return

    if args.action in ('fetch', 'report'):
        from playwright.sync_api import sync_playwright
        sys.path.insert(0, str(Path(__file__).parent))

        with sync_playwright() as pw:
            from xhs_auto import create_browser_context, check_login
            headless = getattr(args, 'headless', False)
            ctx = create_browser_context(pw, headless=headless)
            page = ctx.pages[0] if ctx.pages else ctx.new_page()

            if not check_login(page):
                print(json.dumps({"success": False, "error": "æœªç™»å½•"}, ensure_ascii=False))
                ctx.close()
                sys.exit(1)

            if args.action == 'fetch':
                notes = fetch_note_engagement(page, limit=args.limit)
                print(json.dumps(notes, ensure_ascii=False, indent=2))
            elif args.action == 'report':
                no_eng = getattr(args, 'no_engagement', False)
                report = generate_daily_report(
                    include_engagement=not no_eng,
                    page=page if not no_eng else None,
                )
                if getattr(args, 'json', False):
                    print(json.dumps(report, ensure_ascii=False, indent=2))
                else:
                    print(format_daily_report(report))

            ctx.close()
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
