#!/usr/bin/env python3
"""
çƒ­ç‚¹æ•°æ®é‡‡é›†æ¨¡å—
æ”¯æŒç™¾åº¦çƒ­æœã€å¤´æ¡çƒ­æ¦œã€Bç«™çƒ­æœï¼Œæ— éœ€ API Keyã€‚
"""

import json
import sys
import time
import urllib.request
import urllib.error
from datetime import datetime
from pathlib import Path

SKILL_DIR = Path(__file__).parent.parent
CACHE_DIR = SKILL_DIR / 'content' / 'trending'
CACHE_DIR.mkdir(parents=True, exist_ok=True)

UA = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
TIMEOUT = 15

# â”€â”€â”€ æ•°æ®æº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SOURCES = {
    'baidu': {'name': 'ç™¾åº¦çƒ­æœ', 'emoji': 'ğŸ”'},
    'toutiao': {'name': 'å¤´æ¡çƒ­æ¦œ', 'emoji': 'ğŸ“°'},
    'bilibili': {'name': 'Bç«™çƒ­æœ', 'emoji': 'ğŸ“º'},
}


def _fetch_json(url, headers=None):
    """é€šç”¨ JSON è¯·æ±‚"""
    hdrs = {'User-Agent': UA}
    if headers:
        hdrs.update(headers)
    req = urllib.request.Request(url, headers=hdrs)
    with urllib.request.urlopen(req, timeout=TIMEOUT) as resp:
        return json.loads(resp.read().decode('utf-8'))


def fetch_baidu():
    """ç™¾åº¦çƒ­æœ"""
    url = 'https://top.baidu.com/api/board?platform=wise&tab=realtime'
    data = _fetch_json(url)
    cards = data.get('data', {}).get('cards', [])
    if not cards:
        return []
    # ç™¾åº¦æ¥å£åµŒå¥—ä¸¤å±‚ content: cards[0].content[0].content[]
    top_content = cards[0].get('content', [])
    items = []
    for entry in top_content:
        if isinstance(entry, dict) and 'content' in entry:
            # åµŒå¥—ç»“æ„: entry.content æ˜¯å®é™…åˆ—è¡¨
            items = entry.get('content', [])
            break
        else:
            items = top_content
            break
    results = []
    for i, item in enumerate(items):
        word = item.get('word', item.get('query', ''))
        if not word:
            continue
        rank = i + 1
        if item.get('isTop'):
            rank = 0  # ç½®é¡¶
        elif item.get('index') is not None:
            rank = item['index'] + 1
        results.append({
            'rank': rank,
            'title': word,
            'url': item.get('url', ''),
            'hot': item.get('hotScore', ''),
            'is_top': item.get('isTop', False),
        })
    return results


def fetch_toutiao():
    """å¤´æ¡çƒ­æ¦œ"""
    url = 'https://www.toutiao.com/hot-event/hot-board/?origin=toutiao_pc'
    data = _fetch_json(url)
    items = data.get('data', [])
    results = []
    for i, item in enumerate(items):
        results.append({
            'rank': i + 1,
            'title': item.get('Title', ''),
            'url': item.get('Url', ''),
            'hot': item.get('HotValue', ''),
            'label': item.get('Label', ''),
        })
    return results


def fetch_bilibili():
    """Bç«™çƒ­æœ"""
    url = 'https://app.bilibili.com/x/v2/search/trending/ranking'
    data = _fetch_json(url)
    items = data.get('data', {}).get('list', [])
    results = []
    for i, item in enumerate(items):
        results.append({
            'rank': i + 1,
            'title': item.get('keyword', ''),
            'show_name': item.get('show_name', ''),
            'icon': item.get('icon', ''),
        })
    return results


FETCHERS = {
    'baidu': fetch_baidu,
    'toutiao': fetch_toutiao,
    'bilibili': fetch_bilibili,
}


# â”€â”€â”€ èšåˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_trending(sources=None, limit=20):
    """
    é‡‡é›†çƒ­ç‚¹æ•°æ®

    Args:
        sources: æ•°æ®æºåˆ—è¡¨ (é»˜è®¤å…¨éƒ¨)ï¼Œå¯é€‰ baidu/toutiao/bilibili
        limit: æ¯ä¸ªæºè¿”å›æ¡æ•°

    Returns:
        dict: {source: {name, emoji, items, fetched_at, error?}}
    """
    if sources is None:
        sources = list(FETCHERS.keys())

    result = {}
    for src in sources:
        if src not in FETCHERS:
            result[src] = {'error': f'ä¸æ”¯æŒçš„æ•°æ®æº: {src}'}
            continue

        info = SOURCES[src]
        try:
            items = FETCHERS[src]()
            result[src] = {
                'name': info['name'],
                'emoji': info['emoji'],
                'items': items[:limit],
                'total': len(items),
                'fetched_at': datetime.now().isoformat(),
            }
        except Exception as e:
            result[src] = {
                'name': info['name'],
                'emoji': info['emoji'],
                'items': [],
                'error': str(e),
                'fetched_at': datetime.now().isoformat(),
            }

    return result


def fetch_all_trending(limit=20):
    """é‡‡é›†æ‰€æœ‰æºçš„çƒ­ç‚¹ï¼Œå¸¦ç¼“å­˜ï¼ˆ5åˆ†é’Ÿå†…ä¸é‡å¤è¯·æ±‚ï¼‰"""
    cache_file = CACHE_DIR / 'latest.json'

    # æ£€æŸ¥ç¼“å­˜
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached = json.load(f)
            cached_at = cached.get('_cached_at', 0)
            if time.time() - cached_at < 300:  # 5 åˆ†é’Ÿç¼“å­˜
                return cached
        except Exception:
            pass

    # é‡‡é›†
    data = fetch_trending(limit=limit)
    data['_cached_at'] = time.time()

    # å†™ç¼“å­˜
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return data


def get_top_topics(limit=10):
    """
    ä»æ‰€æœ‰çƒ­æ¦œä¸­æå–å»é‡åçš„çƒ­é—¨è¯é¢˜ï¼ˆç”¨äºå†…å®¹åˆ›ä½œçµæ„Ÿï¼‰

    Returns:
        list[dict]: [{title, source, rank}]
    """
    data = fetch_all_trending(limit=50)
    seen = set()
    topics = []

    for src in ['baidu', 'toutiao', 'bilibili']:
        info = data.get(src, {})
        if info.get('error'):
            continue
        for item in info.get('items', []):
            title = item.get('title', '').strip()
            if not title or title in seen:
                continue
            seen.add(title)
            topics.append({
                'title': title,
                'source': SOURCES[src]['name'],
                'source_key': src,
                'rank': item.get('rank', 0),
            })

    return topics[:limit]


def format_trending_text(data, limit=10):
    """æ ¼å¼åŒ–çƒ­ç‚¹æ•°æ®ä¸ºå¯è¯»æ–‡æœ¬"""
    lines = []
    for src in ['baidu', 'toutiao', 'bilibili']:
        info = data.get(src)
        if not info or info.get('error'):
            if info:
                lines.append(f"{SOURCES[src]['emoji']} {SOURCES[src]['name']}ï¼šè·å–å¤±è´¥ ({info.get('error', 'æœªçŸ¥')})")
            continue

        lines.append(f"\n{info['emoji']} {info['name']}ï¼ˆå…± {info['total']} æ¡ï¼‰")
        for item in info['items'][:limit]:
            rank = item.get('rank', '')
            title = item.get('title', '')
            hot = item.get('hot', '')
            hot_str = f'  ğŸ”¥{hot}' if hot else ''
            prefix = '  ğŸ“Œ' if item.get('is_top') else f'  {rank}.'
            lines.append(f"{prefix} {title}{hot_str}")

    return '\n'.join(lines)


# â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    import argparse

    parser = argparse.ArgumentParser(description='çƒ­ç‚¹æ•°æ®é‡‡é›†')
    parser.add_argument('action', choices=['fetch', 'topics', 'sources'],
                        help='æ“ä½œ: fetch=é‡‡é›†çƒ­æ¦œ, topics=æå–è¯é¢˜, sources=åˆ—å‡ºæ•°æ®æº')
    parser.add_argument('--source', '-s', action='append', dest='sources',
                        help='æ•°æ®æº (å¯å¤šæ¬¡æŒ‡å®š): baidu/toutiao/bilibili')
    parser.add_argument('--limit', '-n', type=int, default=20, help='æ¯æºè¿”å›æ¡æ•° (é»˜è®¤20)')
    parser.add_argument('--no-cache', action='store_true', help='è·³è¿‡ç¼“å­˜')
    parser.add_argument('--json', action='store_true', help='è¾“å‡º JSON æ ¼å¼')

    args = parser.parse_args()

    if args.action == 'sources':
        for key, info in SOURCES.items():
            print(f"  {info['emoji']} {key} â€” {info['name']}")
        return

    if args.action == 'topics':
        topics = get_top_topics(limit=args.limit)
        if args.json:
            print(json.dumps(topics, ensure_ascii=False, indent=2))
        else:
            print(f"ğŸ”¥ çƒ­é—¨è¯é¢˜ Top {len(topics)}:\n")
            for i, t in enumerate(topics, 1):
                print(f"  {i}. {t['title']}  ({t['source']})")
        return

    # fetch
    if args.no_cache:
        data = fetch_trending(sources=args.sources, limit=args.limit)
    else:
        data = fetch_all_trending(limit=args.limit)
        if args.sources:
            data = {k: v for k, v in data.items() if k in args.sources or k.startswith('_')}

    if args.json:
        print(json.dumps(data, ensure_ascii=False, indent=2))
    else:
        print(format_trending_text(data, limit=args.limit))


if __name__ == '__main__':
    main()
